# UnsuperBinTrans

This repository was created for the following paper.

"Unsupervised Binary Code Translation with Application to Code Similarity Detection and Vulnerability Discovery"

Iftakhar Ahmad (University of South Carolina), Lannan Luo (George Mason University).

The source code, datasets, and trained model will be released soon.

# UnsuperBinTrans

[Word2Vec](https://code.google.com/archive/p/word2vec/)

Modify **./demo-word.sh** file in Word2Vec with appropriate input file of monolingual text corpus. Set parameters "size" to 200, "binary" to 0, and "output" to appropriate output file name with ".txt" as file extension.  
&nbsp;

[fasttext](https://github.com/facebookresearch/fastText)

Install fastText following the guidelines from the above GitHub repository.
Use below command to generate monolingual word embeddings for each of x86 and ARM instructions.

    ./fasttext skipgram -input MONOLINGUAL_TEXT_CORPORA.txt -output /path/to/output/directory -minn 2 -maxn 5 -dim 200
&nbsp;

[VecMap](https://github.com/artetxem/vecmap)

Use below command to generate cross-lingual embeddings from monolingual embeddings by VecMap:

    python3 map_embeddings.py --unsupervised SRC.EMB TRG.EMB SRC_MAPPED.EMB TRG_MAPPED.EMB

Here, SRC.EMB is source monolingual embedding which in our case is ARM embedding generated by Word2Vec from ARM text corpus, and TRG.EMB are target monolingual embedding which in our case is x86 embedding generated by Word2Vec from x86 text corpus. SRC_MAPPED.EMB TRG_MAPPED.EMB are cross-lingual embeddings generated by VecMap from two given monolingual text corpus.  
&nbsp;

[UNdreaMT](https://github.com/artetxem/undreamt)

Use below command to train the UNdreaMT model:
    
    python3 train.py --src SRC.MONO.TXT --trg TRG.MONO.TXT --src_embeddings SRC.EMB.TXT --trg_embeddings TRG.EMB.TXT --save MODEL_PREFIX --cuda

Here, SRC.MONO.TXT and TRG.MONO.TXT are source and target monolingual text corpus respectively. In our case source is ARM and target is x86 text corpus. These are same monolingual text corpus used for Word2Vec. SRC.EMB.TXT and TRG.EMB.TXT are source and target cross-lingual embeddings respectively, generated by VecMap. MODEL_PREFIX is the name of the model that will be used to save the trained output model, --cuda tells the model to use GPU for training.  

After the training completes use below command to translate ARM functions to x86 functions:

    python3 translate.py MODEL_PREFIX.final.src2trg.pth < INPUT.TXT > OUTPUT.TXT

Here, MODEL_PREFIX.final.src2trg.pth is the final trained model with appropriate extension. INPUT.TXT is an input file with an ARM function and OUTPUT.TXT will contain the translated version for that ARM function. A bash script TODO is used to translate a list of ARM function to x86 functions.

We have uploaded our dataset to this link [dataset](https://drive.google.com/drive/folders/1AFPha3uPWhnZOY65XcOqPk4JDlppOR55)

